# ğŸ¤– BomBot - Self-Hosted AI SBOM Security Analysis Platform

> **Next-Generation Security Analysis**  
> A hybrid-architecture full-stack security platform with **self-hosted Llama 3.2 AI** that provides instant vulnerability analysis, intelligent function calling, and comprehensive SBOM security assessment with complete data privacy.


## ğŸ—ï¸ System Architecture

BomBot implements a **self-hosted AI architecture** with direct chat completions and intelligent function calling for comprehensive security analysis:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 BomBot Self-Hosted Platform                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ¨ React UI (Vite)              ğŸ”§ Next.js API Routes     â”‚
â”‚  â”œâ”€â”€ ChatInterface.tsx           â”œâ”€â”€ /api/chat             â”‚
â”‚  â”œâ”€â”€ Direct AI Chat              â”œâ”€â”€ /api/upload           â”‚
â”‚  â”œâ”€â”€ ChatMessage.tsx             â”œâ”€â”€ /api/osv-query        â”‚
â”‚  â”œâ”€â”€ VulnerabilityCards          â””â”€â”€ /api/run-status       â”‚
â”‚  â””â”€â”€ Conversation History                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ§  AI Client Engine (src/lib/ai-client.ts)               â”‚
â”‚  â”œâ”€â”€ Function Call Detection     â”œâ”€â”€ Conversation Context  â”‚
â”‚  â”œâ”€â”€ Hybrid Provider Support     â”œâ”€â”€ Direct Responses     â”‚
â”‚  â””â”€â”€ Manual Function Execution   â””â”€â”€ Error Recovery       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          AI & Data Integrations                   â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  ğŸš€ Jetstream (Self-Hosted)   ğŸ“Š OSV.dev API      â”‚
    â”‚  â”œâ”€â”€ Llama 3.2-1B-Instruct    â”œâ”€â”€ Package Queries â”‚
    â”‚  â”œâ”€â”€ Direct Chat Completions  â”œâ”€â”€ CVE Lookups     â”‚
    â”‚  â”œâ”€â”€ Complete Data Privacy    â””â”€â”€ Real-time Data  â”‚
    â”‚  â””â”€â”€ Custom Prompting                             â”‚
    â”‚                                                   â”‚
    â”‚  ğŸ”„ OpenAI Fallback (Optional)                    â”‚
    â”‚  â””â”€â”€ GPT-4 Compatibility                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Advanced Features

### ğŸš€ **Self-Hosted AI Architecture**
- **Direct Chat Completions**: Instant responses from your own Llama 3.2 model
- **Complete Data Privacy**: All AI processing stays on your infrastructure
- **Intelligent Function Calling**: Automatic package/CVE query detection and execution
- **Conversation Context**: Maintains chat history for better AI responses
- **Hybrid Provider Support**: Jetstream primary, OpenAI fallback capability

### ğŸ¯ **Intelligent Package Analysis**
- **Multi-Ecosystem Support**: npm, PyPI, Maven, Go, NuGet, RubyGems, Cargo, Composer, Hex, SwiftPM
- **Version-Specific Queries**: Precise vulnerability matching for specific package versions
- **Severity Intelligence**: CVSS score parsing to clean severity levels (CRITICAL/HIGH/MEDIUM/LOW)
- **CVE Cross-Reference**: Direct OSV.dev integration for authoritative vulnerability data

### ğŸ›¡ï¸ **SBOM Processing Engine**
- **Multi-Format Support**: SPDX, CycloneDX, Generic JSON schemas
- **Batch Processing**: Concurrent vulnerability scanning with rate limiting
- **Dependency Mapping**: Package ecosystem detection from PURL and metadata
- **Scalable Architecture**: Handles up to 50 packages per SBOM (serverless optimization)

### ğŸ’¬ **Advanced Chat System**
- **Direct Responses**: Immediate AI responses without polling or threads
- **Conversation History**: Smart context management for multi-turn conversations
- **Markdown Rendering**: Rich formatted responses with proper spacing
- **File Upload Integration**: Drag-and-drop SBOM processing with instant AI analysis

### ğŸ” **Smart Vulnerability Cards**
- **Interactive UI**: Clickable cards with direct OSV.dev links
- **Severity Visualization**: Color-coded badges with appropriate icons
- **Package Context**: Version-aware vulnerability attribution
- **Export Ready**: Structured data for reporting and integration

## ğŸ› ï¸ Technical Implementation

### Frontend Architecture (React + TypeScript)

#### **Core Components**
```typescript
// ChatInterface.tsx - Main orchestration component
- File upload handling (drag-and-drop + file picker)
- Direct AI chat integration with conversation history
- Instant response handling (no polling required)
- Smart function call detection and execution

// AI Client (src/lib/ai-client.ts) - Unified AI interface
- Jetstream and OpenAI provider support
- Automatic function call detection for packages/CVEs
- Manual function execution for Llama 3.2
- Conversation context management

// ChatMessage.tsx - Dynamic message rendering
- Markdown rendering with syntax highlighting
- Vulnerability card generation from AI responses
- Copy-to-clipboard functionality
- Responsive design with accessibility

// VulnerabilityCard Component
- Interactive severity badges with color coding
- Direct OSV.dev linking (not NVD)
- Package version display with ecosystem info
- Description truncation with smart expansion
```

#### **State Management**
```typescript
// ChatContext.tsx - Centralized state with conversation history
interface ChatState {
  messages: Message[];
  conversationHistory: Array<{role: 'user' | 'assistant', content: string}>;
  isLoading: boolean;
  currentThreadId: string | null;
  uploadedFiles: UploadedFile[];
  userEmail: string | null;
}

interface Message {
  id: string;
  type: 'user' | 'assistant';
  content: string;
  timestamp: Date;
  vulnerabilities?: Vulnerability[];
  useMarkdown?: boolean; // Controls rendering strategy
}
```

#### **Severity Processing Pipeline**
```typescript
// Enhanced severity extraction with multiple fallbacks
function extractSeverity(vuln: OSVVulnerability): string {
  // 1. CVSS v3 score parsing (primary)
  if (vuln.severity?.find(s => s.type === 'CVSS_V3')) {
    const score = parseFloat(cvssVector.split('/')[0]);
    return scoreToBracket(score); // 9.0+ = CRITICAL, 7.0+ = HIGH, etc.
  }
  
  // 2. Database-specific severity (GHSA, etc.)
  if (vuln.database_specific?.severity) {
    return vuln.database_specific.severity.toUpperCase();
  }
  
  // 3. Content analysis fallback
  return analyzeSeverityFromContent(vuln.summary, vuln.details);
}
```

### Backend Architecture (Next.js API)

#### **API Route Specifications**

##### `/api/upload` - SBOM Processing Pipeline
```typescript
interface UploadFlow {
  1. File validation (type, size, structure)
  2. Multi-format SBOM parsing (SPDX/CycloneDX/Generic)
  3. Package extraction with ecosystem mapping
  4. Batch OSV API queries (rate-limited)
  5. Vulnerability aggregation and severity normalization
  6. Direct AI analysis with structured SBOM data
  7. Instant AI response generation
  8. Conversation ID creation for follow-up chats
}

// Response includes AI analysis and structured data
interface UploadResponse {
  success: boolean;
  conversationId: string;
  aiResponse: string; // Direct AI analysis
  vulnerabilitiesFound: number;
  quickSummary: QuickSummaryData;
  dependencyGraph: DependencyGraph;
}
```

##### `/api/chat` - Direct AI Chat Completions
```typescript
interface ChatAPI {
  directResponses: boolean; // No polling required
  conversationHistory: Array<{role: string, content: string}>;
  functionCalling: {
    packageDetection: boolean; // Auto-detects package queries
    cveDetection: boolean;     // Auto-detects CVE mentions
    automaticExecution: boolean; // Executes OSV queries
  };
  
  providers: {
    jetstream: JetstreamConfig; // Primary self-hosted
    openai: OpenAIConfig;       // Fallback option
  };
}
```

##### `/api/osv-query` - Package Intelligence
```typescript
interface QueryCapabilities {
  packageQueries: {
    ecosystems: string[]; // 11 supported ecosystems
    versionSupport: boolean;
    realTimeData: boolean;
  };
  
  cveQueries: {
    directLookup: boolean;
    aliasResolution: boolean;
    affectedPackages: boolean;
  };
  
  // Simplified - no AI integration (handled by chat API)
  directDataReturn: boolean;
}
```

##### `/api/run-status` - Legacy Compatibility
```typescript
// Deprecated - kept for backward compatibility
interface CompatibilityEndpoint {
  status: 'deprecated';      // Always returns completed
  message: 'This endpoint is deprecated. Use /api/chat for direct responses.';
}
```

#### **Self-Hosted AI Integration**

##### **Function Call Detection**
```typescript
interface FunctionDetection {
  packageQueries: {
    patterns: RegExp[]; // Detects "Is X safe?", "check package Y"
    extraction: {
      packageName: string;
      version?: string;
      ecosystem: string; // Auto-detected or default npm
    };
  };
  
  cveQueries: {
    pattern: /CVE-\d{4}-\d{4,}/gi;
    directExecution: boolean; // Immediate OSV lookup
  };
}
```

##### **AI Client Configuration**
```typescript
interface AIClientConfig {
  provider: 'jetstream' | 'openai';
  jetstream: {
    baseUrl: string;        // Your Jetstream server
    apiKey: string;         // Authentication
    model: string;          // Llama model name
    temperature: 0.7;       // Balanced creativity
    max_tokens: 2048;       // Response length
  };
  openai: {
    model: "gpt-4";
    temperature: 0.1;       // High consistency for security
    max_tokens: 2048;
  };
  systemPrompt: string;     // From Instruction Prompt.md
}
```

## ğŸ”§ Configuration & Deployment

### Environment Variables
```bash
# AI Provider Configuration (Choose One)
MODEL_PROVIDER=jetstream               # Use 'jetstream' or 'openai'

# Jetstream Configuration (Self-Hosted Llama)
JETSTREAM_API_KEY=sk-xxxxx            # Your Jetstream API key
JETSTREAM_BASE_URL=http://server:8080  # Your Jetstream server URL  
JETSTREAM_MODEL=meta-llama/Llama-3.2-1B-Instruct

# OpenAI Configuration (Fallback)
OPENAI_API_KEY=sk-proj-xxxxx          # OpenAI API access (optional)
# Database Configuration (Required)
NEXT_PUBLIC_SUPABASE_URL=https://xxx.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=eyJxxx
SUPABASE_SERVICE_ROLE_KEY=eyJxxx

# Optional - Development
OSV_SCANNER_PATH=/usr/local/bin/osv-scanner  # Local binary path
NODE_ENV=production                     # Runtime environment
```

### Self-Hosted AI Setup

Your **Llama 3.2-1B-Instruct** model is configured with the comprehensive system prompt from `Instruction Prompt.md`, providing:

```yaml
AI Configuration:
  model: "meta-llama/Llama-3.2-1B-Instruct"
  provider: "jetstream" 
  temperature: 0.7
  max_tokens: 2048
  
Core Capabilities:
  1. Real-time vulnerability research using OSV.dev database
  2. Intelligent function call detection (packages/CVEs)
  3. Manual function execution for Llama compatibility
  4. Comprehensive security analysis with business impact
  5. Conversation context preservation across messages
  
Data Privacy:
  - All AI processing on your infrastructure
  - No external API calls for chat completions
  - Complete control over model behavior
  - Custom fine-tuning possible
  
Function Detection:
  - Package queries: "Is lodash safe?", "check express 4.17.1"
  - CVE queries: "Tell me about CVE-2023-1234"
  - Automatic OSV API integration
  - Instant response delivery (no polling)

Fallback Support:
  - OpenAI GPT-4 available if needed
  - Seamless provider switching
  - Same function calling capabilities
```

## ğŸ“Š Performance Metrics

### Bundle Analysis
```
Production Build:
â”œâ”€â”€ UI Assets
â”‚   â”œâ”€â”€ JavaScript: 520KB â†’ 163KB gzipped (-69%)
â”‚   â”œâ”€â”€ CSS: 90KB â†’ 14KB gzipped (-84%)
â”‚   â””â”€â”€ Total: 177KB gzipped (optimized for features)
â”‚
â”œâ”€â”€ API Routes
â”‚   â”œâ”€â”€ Shared Runtime: 85KB (includes AI client)
â”‚   â”œâ”€â”€ Cold Start: <2s on Vercel
â”‚   â””â”€â”€ Memory Usage: ~140MB per function
â”‚
â””â”€â”€ Dependencies
    â”œâ”€â”€ React/UI: ~40KB gzipped
    â”œâ”€â”€ AI Client: ~25KB gzipped (hybrid support)
    â”œâ”€â”€ Form Processing: ~25KB gzipped
    â””â”€â”€ Markdown Rendering: ~20KB gzipped
```

### Runtime Performance
```
Response Times (95th percentile):
â”œâ”€â”€ Package Queries: <500ms (OSV API latency)
â”œâ”€â”€ SBOM Processing: 2-8s (size dependent)
â”œâ”€â”€ AI Chat Responses: 1-5s (direct completion)
â”œâ”€â”€ Function Calls: <2s (OSV API + AI processing)
â””â”€â”€ UI Interactions: <100ms (client-side)

Scalability Limits:
â”œâ”€â”€ File Size: 10MB (Vercel function limit)
â”œâ”€â”€ SBOM Packages: 150 (rate limiting optimization)
â”œâ”€â”€ Concurrent Users: Unlimited (serverless)
â”œâ”€â”€ Jetstream: Your server capacity dependent
â””â”€â”€ AI Rate Limits: Self-hosted = no limits
```

## ğŸš€ Usage Patterns

### Enterprise SBOM Analysis
```typescript
// Typical enterprise workflow
1. Upload company SBOM file (SPDX/CycloneDX)
2. Receive instant vulnerability summary with priority rankings
3. Review interactive vulnerability cards with OSV.dev links
4. Ask AI: "What should we prioritize for our next sprint?"
5. Get detailed remediation plan with business impact assessment
6. Export findings for security team review
```

### Developer Package Research
```typescript
// Developer security research workflow  
1. Query specific package: "Is express 4.17.1 safe?"
2. Get immediate vulnerability status with severity badges
3. Ask follow-up: "What about the latest version?"
4. Receive comparative analysis with upgrade recommendations
5. Deep-dive: "Give me detailed analysis of these vulnerabilities"
6. Get technical implementation guidance
```

## ğŸ”„ Development Workflow

### Local Development Setup
```bash
# Complete development environment
git clone https://github.com/kakderishikesh/BomBot-Chat.git
cd BomBot-Chat

# Install dependencies
npm install

# Environment setup
cp .env.example .env
# Configure your AI provider:
# For Jetstream: MODEL_PROVIDER, JETSTREAM_API_KEY, etc.
# For OpenAI: OPENAI_API_KEY (fallback)

# Optional: Install OSV Scanner locally
brew install osv-scanner  # macOS

# Start development servers
npm run dev          # Full-stack development (recommended)
```

### Development Commands
```bash
# Development
npm run dev          # Full-stack with hot reload
npm run dev:ui       # UI development server only

# Building
npm run build        # Production build (UI + API)
npm run build:ui     # UI build only
npm run build:api    # API build only

# Quality Assurance
npm run type-check   # TypeScript validation
npm run lint         # Code quality checks
```

## ğŸ” Security Features

### Data Handling
```typescript
Security Measures:
â”œâ”€â”€ File Processing
â”‚   â”œâ”€â”€ Temporary file creation in isolated containers
â”‚   â”œâ”€â”€ Automatic cleanup after processing
â”‚   â”œâ”€â”€ Size and type validation (10MB limit)
â”‚   â””â”€â”€ No persistent storage of uploaded content
â”‚
â”œâ”€â”€ API Security  
â”‚   â”œâ”€â”€ Environment variable encryption (Vercel)
â”‚   â”œâ”€â”€ No API key exposure to client
â”‚   â”œâ”€â”€ Request validation and sanitization
â”‚   â””â”€â”€ Rate limiting on OSV API calls
â”‚
â”œâ”€â”€ OpenAI Integration
â”‚   â”œâ”€â”€ Isolated thread creation per session
â”‚   â”œâ”€â”€ Function call validation
â”‚   â””â”€â”€ Response content filtering
â”‚
â””â”€â”€ Client-Side Security
    â”œâ”€â”€ Secure API communication (HTTPS)
    â”œâ”€â”€ Input validation on all forms
    â””â”€â”€ XSS protection via React's built-in escaping
```

## ğŸ“ˆ Key Features Summary

### âœ… **Current Capabilities**
- **Hybrid Response System**: Instant templated responses + AI deep analysis
- **Multi-Format SBOM Support**: SPDX, CycloneDX, Generic JSON
- **11 Package Ecosystems**: npm, PyPI, Maven, Go, NuGet, RubyGems, etc.
- **Real-time OSV.dev Integration**: Authoritative vulnerability data
- **Clean Severity Tags**: CRITICAL/HIGH/MEDIUM/LOW (not CVSS vectors)
- **Interactive Vulnerability Cards**: Clickable, exportable, OSV.dev linked
- **Persistent AI Conversations**: Thread-based context preservation
- **Markdown Rich Responses**: Properly formatted AI analysis
- **Drag-and-Drop File Upload**: 10MB limit with progress tracking
- **Silent Polling**: No timeout interruptions for better UX

### ğŸ”§ **Technical Stack**
- **Frontend**: React 18 + TypeScript + Vite + TailwindCSS + Radix UI
- **Backend**: Next.js 14 + TypeScript + OpenAI API + OSV.dev API
- **Deployment**: Vercel Serverless with optimized build pipeline
- **AI**: GPT-4 Turbo with custom security analyst assistant
- **State Management**: React Context with custom hooks
- **File Processing**: Multi-format parsing with batch vulnerability scanning

---

## ğŸ Quick Start

1. **Deploy**: Use Vercel or similar platform for deployment
2. **Configure**: Add `OPENAI_API_KEY` and `ASSISTANT_ID` to Vercel environment
3. **Use**: Upload SBOM files and start chatting with your AI security expert!

---

## ğŸ“„ License

This project is licensed under the **Mozilla Public License 2.0** - see the [LICENSE](LICENSE) file for details.

### Key License Features:
- **Open Source**: Free to use, modify, and distribute
- **Copyleft**: Modifications must be shared under the same license
- **Patent Protection**: Includes patent licensing provisions
- **Commercial Use**: Allowed with proper attribution
- **Compatible**: Works with many other open source licenses

For more information about MPL-2.0, visit: https://mozilla.org/MPL/2.0/

---

*Last Updated: July 2025 | Version: 2.0.0 | Build: d1bfa0a*
